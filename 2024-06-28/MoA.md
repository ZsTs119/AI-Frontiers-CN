模型论文地址：https://arxiv.org/abs/2406.14909

模型概述：标题：MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression
中文文章标题：MoA：用于自动大型语言模型压缩的稀疏注意力混合
文章内容：提出MoA方法，通过稀疏注意力机制自动压缩大型语言模型，提高效率并保持性能。
