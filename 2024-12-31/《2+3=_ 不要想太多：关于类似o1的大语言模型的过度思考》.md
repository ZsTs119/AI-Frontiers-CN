模型论文地址：https://arxiv.org/abs/2412.21187

模型概述：当前标题：《Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs》
中文文章标题：《2+3=? 不要想太多：关于类似o1的大语言模型的过度思考》
文章内容：本文探讨了类似o1的大语言模型在处理简单问题时出现的过度思考现象，可能导致错误答案。
