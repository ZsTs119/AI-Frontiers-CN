模型论文地址：https://arxiv.org/abs/2406.19774

模型概述：当前标题：《Direct Preference Knowledge Distillation for Large Language Models》
中文文章标题：《面向大型语言模型的直接偏好知识蒸馏》
文章内容：提出直接偏好知识蒸馏方法，通过用户偏好指导大型语言模型训练，提高模型性能及效率。
