模型论文地址：https://arxiv.org/abs/2503.02130

模型概述：文章标题：《Forgetting Transformer: Softmax Attention with a Forget Gate》
中文文章标题：《遗忘Transformer：带遗忘门的Softmax注意力》
文章内容：提出一种新的Transformer架构，通过引入遗忘门机制改进Softmax注意力，提高模型对长期依赖的建模能力。
