模型论文地址：https://arxiv.org/abs/2406.05955

模型概述：当前标题：《Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters》
中文文章标题：《Turbo Sparse：通过最小激活参数实现LLM SOTA性能》
文章内容：提出Turbo Sparse方法，通过最小化激活参数，实现大规模语言模型LLM的SOTA性能。
