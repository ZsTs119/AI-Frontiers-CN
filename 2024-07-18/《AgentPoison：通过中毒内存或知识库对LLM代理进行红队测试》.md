模型论文地址：https://arxiv.org/abs/2407.12784

模型概述：当前标题：《AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases》
中文文章标题：《AgentPoison：通过中毒内存或知识库对LLM代理进行红队测试》
文章内容：提出AgentPoison方法，通过毒化LLM的内存或知识库进行安全性测试，增强大型语言模型代理的鲁棒性。
