模型论文地址：https://arxiv.org/abs/2407.11062

模型概述：文章标题《EfficientQAT: Efficient Quantization-Aware Training for Large Language Models》
中文文章标题《EfficientQAT：用于大型语言模型的有效量化感知训练》
文章内容：提出EfficientQAT方法，通过量化感知训练提升大型语言模型效率，减少计算和存储需求。
