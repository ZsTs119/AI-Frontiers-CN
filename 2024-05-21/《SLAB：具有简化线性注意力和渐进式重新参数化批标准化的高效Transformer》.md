模型论文地址：https://arxiv.org/abs/2405.11582

模型概述：文章标题：《SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization》
中文文章标题：《SLAB：具有简化线性注意力和渐进式重新参数化批标准化的高效Transformer》
文章内容：提出SLAB模型，通过简化线性注意力和渐进式重新参数化批标准化提高Transformer效率，实现更快的训练速度和更好的性能。
