模型论文地址：https://arxiv.org/abs/2502.11089

模型概述：文章标题：《Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention》
中文文章标题：《原生稀疏注意力：硬件对齐和原生可训练的稀疏注意力》
文章内容：提出原生稀疏注意力机制，与硬件对齐，提高计算效率，实现原生可训练性。
