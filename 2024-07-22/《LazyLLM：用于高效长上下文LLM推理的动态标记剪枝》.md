模型论文地址：https://arxiv.org/abs/2407.14057

模型概述：标题：《LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference》
中文文章标题：《LazyLLM：用于高效长上下文LLM推理的动态标记剪枝》
文章内容：提出LazyLLM方法，通过动态剪枝减少长上下文中无关token，提升LLM推理效率。
