模型论文地址：https://arxiv.org/abs/2405.12981

模型概述：文章标题：《Reducing Transformer Key-Value Cache Size with Cross-Layer Attention》
中文文章标题：《通过跨层注意力减少Transformer键值缓存大小》
文章内容：提出一种新型注意力机制，通过跨层信息传递减少Transformer模型中键值缓存的需求，提高计算效率。
