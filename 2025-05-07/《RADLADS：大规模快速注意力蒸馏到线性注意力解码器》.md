模型论文地址：https://arxiv.org/abs/2505.03005

模型概述：当前标题：《RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale》
中文文章标题：《RADLADS：大规模快速注意力蒸馏到线性注意力解码器》
文章内容：提出RADLADS方法，通过快速注意力蒸馏，将大规模模型中的注意力机制转移到线性注意力解码器，提升计算效率。
