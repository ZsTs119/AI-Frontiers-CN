模型论文地址：https://arxiv.org/abs/2503.02130

模型概述：文章标题：《Forgetting Transformer: Softmax Attention with a Forget Gate》
中文文章标题：《遗忘Transformer：带有遗忘门的Softmax注意力机制》
文章内容：提出遗忘Transformer，通过引入遗忘门改进Softmax注意力，增强模型对不重要信息的忽略能力，提高注意力效率。
