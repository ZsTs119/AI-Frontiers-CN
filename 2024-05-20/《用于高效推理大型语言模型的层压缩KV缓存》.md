模型论文地址：https://arxiv.org/abs/2405.10637

模型概述：标题：《Layer-Condensed KV Cache for Efficient Inference of Large Language Models》
中文文章标题：《用于高效推理大型语言模型的层压缩KV缓存》
文章内容：提出层压缩KV缓存方法，通过减少缓存大小和提升缓存利用率，加速大型语言模型推理。
