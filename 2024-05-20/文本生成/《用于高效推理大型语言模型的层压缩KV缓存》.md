# Layer-Condensed KV Cache - Layer-Condensed KV Cache for Efficient Inference of Large Language Models

**模型功能**：
该模型提出了层压缩KV缓存方法，通过减少缓存大小和提升缓存利用率，来加速大型语言模型的推理过程。

**arXiv 文章链接**：
[https://arxiv.org/abs/2405.10637](https://arxiv.org/abs/2405.10637)

**作者/团队**：
待通过论文页面进一步查看获取准确作者姓名或团队名称。

**发表日期**：
待通过论文页面获取论文的首次提交或发表日期。

**研究进展**：
该模型的创新点在于提出了层压缩KV缓存方法，此方法能够减少缓存大小以及提升缓存利用率，从而在大型语言模型推理方面实现加速，提高推理效率。在文本生成相关的大型语言模型推理技术上有一定的进展和优化。

**应用场景**：
在自动写作、自动翻译、聊天机器人、文章续写、新闻撰写、剧本生成等文本生成应用场景中，使用该模型可加速大型语言模型的推理过程，提高这些应用的响应速度和处理效率。