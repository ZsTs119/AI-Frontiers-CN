模型论文地址：https://arxiv.org/abs/2506.14435

模型概述：文章标题：《MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models》
中文文章标题：《MoTE：用于内存高效的大型多模态模型的二元专家混合》
文章内容：提出MoTE模型，通过三元专家混合实现内存高效的大型多模态模型，减少存储需求并提升计算效率。
