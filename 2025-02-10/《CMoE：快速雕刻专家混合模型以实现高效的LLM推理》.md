模型论文地址：https://arxiv.org/abs/2502.04416

模型概述：文章标题《CMoE: Fast Carving of Mixture-of-Experts for Efficient LLM Inference》，
中文文章标题《CMoE：快速雕刻专家混合模型以实现高效的LLM推理》，
文章内容：提出CMoE方法，通过快速雕刻专家混合模型，减少模型大小和推理延迟，提高大型语言模型推理效率。
