模型论文地址：https://arxiv.org/abs/2502.07617

模型概述：标题：《Scaling Pre-training to One Hundred Billion Data for Vision Language Models》
中文文章标题：《将预训练扩展到一千亿数据以用于视觉语言模型》
文章内容：提出了一种大规模视觉语言模型，通过在一千亿级数据上预训练，显著提高了模型性能和泛化能力。
