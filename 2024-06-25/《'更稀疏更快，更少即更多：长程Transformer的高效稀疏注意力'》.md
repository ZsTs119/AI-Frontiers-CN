模型论文地址：https://arxiv.org/abs/2406.16747

模型概述：文章标题《'Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers'》，
中文文章标题《'更稀疏更快，更少即更多：长程Transformer的高效稀疏注意力'》，
文章内容：['提出高效稀疏注意力机制，通过减少计算和内存需求，加速长程Transformer模型训练与推理。']
