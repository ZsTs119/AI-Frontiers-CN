模型论文地址：https://arxiv.org/abs/2506.05629

模型概述：文章标题：《Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs》
中文文章标题：《利用自注意力机制在LLMs中进行输入依赖的软提示》
文章内容：提出一种输入依赖的软提示方法，通过自注意力机制增强大型语言模型性能，提高生成质量。
