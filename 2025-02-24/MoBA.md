模型论文地址：https://arxiv.org/abs/2502.13189

模型概述：标题：MoBA: Mixture of Block Attention for Long-Context LLMs
中文文章标题：MoBA：长上下文LLM的块注意力混合
文章内容：提出MoBA模型，通过块注意力机制优化长上下文处理，提高大型语言模型效率。
