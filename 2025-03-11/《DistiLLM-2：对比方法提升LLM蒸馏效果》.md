模型论文地址：https://arxiv.org/abs/2503.07067

模型概述：当前标题：《DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs》
中文文章标题：《DistiLLM-2：对比方法提升LLM蒸馏效果》
文章内容：提出DistiLLM-2方法，通过对比学习提升大型语言模型蒸馏效果，提高学生模型的性能和效率。
