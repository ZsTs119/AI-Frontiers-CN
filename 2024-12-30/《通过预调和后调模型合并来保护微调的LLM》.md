模型论文地址：https://arxiv.org/abs/2412.19512

模型概述：当前标题：《Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging》
中文文章标题：《通过预调和后调模型合并来保护微调的LLM》
文章内容：提出一种通过预调和后调合并模型的方法，增强大型语言模型的安全性，减少对抗攻击的影响。
