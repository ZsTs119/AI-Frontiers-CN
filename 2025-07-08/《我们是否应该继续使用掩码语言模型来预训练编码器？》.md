模型论文地址：https://arxiv.org/abs/2507.00994

模型概述：当前标题：《Should We Still Pretrain Encoders with Masked Language Modeling?》
中文文章标题：《我们是否应该继续使用掩码语言模型来预训练编码器？》
文章内容：探讨了掩码语言模型在预训练编码器中的有效性，发现其对于提升模型性能的作用有限。
