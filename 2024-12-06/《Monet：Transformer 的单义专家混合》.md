模型论文地址：https://arxiv.org/abs/2412.04139

模型概述：文章标题：《Monet: Mixture of Monosemantic Experts for Transformers》
中文文章标题：《Monet：Transformer 的单义专家混合》
文章内容：提出Monet模型，通过单义专家混合提升Transformer性能，实现更高效的计算和注意力分配。
