# [未提及] - [Data Mixing Made Efficient: A Bivariate Scaling Law for Language Model Pretraining]

**模型功能**：该模型提出了一种新的语言模型预训练方法，借助双变量缩放法则实现数据混合的高效性，从而显著提升预训练模型的性能，本质上是为文本生成相关的语言模型在预训练阶段提供优化手段，以提高后续文本生成等能力。

**arXiv 文章链接**：
[https://arxiv.org/abs/2405.14908](https://arxiv.org/abs/2405.14908)

**作者/团队**：论文作者为 Yuxin Wen、Yifan Xu、Yuzhe Yang、Xupeng Miao、Xiangning Chen、Shuchang Zhou、Yingyan Lin。

**发表日期**：2024年5月26日

**研究进展**：该研究创新性地提出了双变量缩放法则用于语言模型预训练中的数据混合，与传统方法相比，能有效提高数据混合的效率，进而提升预训练模型的性能，为语言模型的预训练提供了新的思路和方法。

**应用场景**：由于是针对语言模型预训练的优化，其应用场景与文本生成领域的应用场景一致，包括自动写作、自动翻译、聊天机器人、文章续写、新闻撰写、剧本生成等。