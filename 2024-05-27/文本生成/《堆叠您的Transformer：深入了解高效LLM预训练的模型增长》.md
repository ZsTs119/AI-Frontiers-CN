# Stacking Your Transformers - Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training

**模型功能**：
该模型通过堆叠Transformer模型来实现高效大型语言模型（LLM）的预训练，旨在探索一种有效的模型增长策略以提升预训练效率。

**arXiv 文章链接**：
[arXiv 链接](https://arxiv.org/abs/2405.15319)

**作者/团队**：
文章作者为Pranav Shyam、Daniele Atzeni、Jiawei Han、Anima Anandkumar。

**发表日期**：
2024年5月24日

**研究进展**：
文章提出了新颖的模型增长策略，聚焦于高效的LLM预训练。通过堆叠Transformer模型，为大型语言模型的预训练提供了新的思路，有望在文本生成相关任务中提高效率和性能。

**应用场景**：
由于该模型是关于大型语言模型预训练的研究，其典型应用场景包括自动写作、自动翻译、聊天机器人、文章续写、新闻撰写、剧本生成等文本生成相关场景。