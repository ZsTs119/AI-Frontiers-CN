模型论文地址：https://arxiv.org/abs/2405.15319

模型概述：《Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training》
中文文章标题：《堆叠您的Transformer：深入了解高效LLM预训练的模型增长》
文章内容：探讨通过堆叠Transformer模型以实现高效大型语言模型预训练，提出新颖的模型增长策略。
