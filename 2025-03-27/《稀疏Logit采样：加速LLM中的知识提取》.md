模型论文地址：https://arxiv.org/abs/2503.16870

模型概述：文章标题：《Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs》
中文文章标题：《稀疏Logit采样：加速LLM中的知识提取》
文章内容：提出稀疏Logit采样方法，通过降低知识提取的计算负担，加速大规模语言模型的知识传递过程。
