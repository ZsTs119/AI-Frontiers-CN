模型论文地址：https://arxiv.org/abs/2507.07015

模型概述：文章标题: MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation
中文文章标题: MST-Distill：用于跨模态知识蒸馏的专业教师混合
文章内容: 提出MST-Distill方法，通过混合专业教师进行跨模态知识蒸馏，提高学生模型性能。
