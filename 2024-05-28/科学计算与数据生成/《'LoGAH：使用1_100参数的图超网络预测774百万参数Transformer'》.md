# LoGAH - LoGAH: Predicting 774-Million-Parameter Transformers using Graph HyperNetworks with 1/100 Parameters

**模型功能**：
论文提出的LoGAH方法利用图超网络，仅使用1/100的参数来预测大规模Transformer模型，在科学计算领域有助于高效地模拟和预测大规模模型，减少计算资源的消耗和参数数量。

**arXiv 文章链接**：
[arXiv 链接](https://arxiv.org/abs/2405.16287)

**作者/团队**：
论文作者有Zhengyang Shen、Jiacheng Yang、Xiaotian Han、Jing Huang、Yizhou Sun、Bo Li。

**发表日期**：
2024年5月27日。

**研究进展**：
该研究提出了一种新的图超网络方法LoGAH，能够以极低的参数（仅为目标模型的1/100）来预测大规模的Transformer模型。这种方法为减少模型参数、提高计算效率提供了新的思路，在科学计算中对于大规模模型的预测和模拟具有创新性，有望在资源受限的环境中实现更高效的模型预测。

**应用场景**：
在科学计算领域，如药物发现、材料设计等场景中，可能需要处理大规模的模型。LoGAH方法可以帮助在资源有限的情况下，对这些大规模Transformer模型进行预测和模拟，减少计算资源的消耗，提高研究和开发的效率。