模型论文地址：https://arxiv.org/abs/2506.11886

模型概述：当前标题：《Beyond Homogeneous Attention: Memory-Efficient LLMs via Fourier-Approximated KV Cache》
中文文章标题：《超越同质注意力：通过傅里叶近似KV缓存实现内存高效的LLMs》
文章内容：提出一种新的内存高效的大型语言模型，通过傅里叶近似技术优化键值缓存，减少内存需求并提高计算效率。
