模型论文地址：https://arxiv.org/abs/2411.17691

模型概述：文章标题：《Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens》
中文文章标题：《低比特量化有利于欠训练的LLM：具有100T训练标记的量化LLM的缩放定律》
文章内容：研究表明低比特量化可提升欠训练LLM性能，揭示了量化LLM的缩放规律。
