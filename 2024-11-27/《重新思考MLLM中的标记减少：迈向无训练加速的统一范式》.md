模型论文地址：https://arxiv.org/abs/2411.17686

模型概述：文章标题：《Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration》
中文文章标题：《重新思考MLLM中的标记减少：迈向无训练加速的统一范式》
文章内容：探讨在多语言大型语言模型中减少标记以加速推理的新方法，提出无训练加速的统一范式。
