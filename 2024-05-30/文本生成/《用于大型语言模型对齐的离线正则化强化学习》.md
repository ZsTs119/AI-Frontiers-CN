# Offline Regularised Reinforcement Learning for Large Language Models Alignment - 《用于大型语言模型对齐的离线正则化强化学习》

**模型功能**：该模型提出一种离线正则化强化学习方法，利用无标签数据来提升大型语言模型的性能和泛化能力，从而使语言模型在文本生成等相关任务中表现更好。

**arXiv 文章链接**：
[https://arxiv.org/abs/2405.19107](https://arxiv.org/abs/2405.19107)

**作者/团队**：文章作者为Shengjie Xu、Rishabh Agarwal、Zihan Zhou、Ofir Nachum、Karthik Narasimhan。

**发表日期**：2024年5月30日

**研究进展**：此研究创新地将离线正则化强化学习应用于大型语言模型对齐问题，利用无标签数据进行训练，突破了传统方法对有标签数据的依赖，提高了模型在实际应用中的泛化能力和性能，为大型语言模型的优化提供了新的思路和方法。

**应用场景**：该模型可应用于自动写作、自动翻译、聊天机器人、文章续写、新闻撰写、剧本生成等文本生成相关的场景，帮助提升语言模型在这些场景下的表现和效果。