模型论文地址：https://arxiv.org/abs/2407.02490

模型概述：文章标题: 'MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention'
中文文章标题: 'MInference 1.0：通过动态稀疏注意力加速长上下文LLM的预填充'
文章内容: 提出MInference 1.0方法，通过动态稀疏注意力优化长上下文LLM的预填充，提高推理速度和效率。
