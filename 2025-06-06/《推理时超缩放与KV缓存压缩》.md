模型论文地址：https://arxiv.org/abs/2506.05345

模型概述：当前标题：《Inference-Time Hyper-Scaling with KV Cache Compression》
中文文章标题：《推理时超缩放与KV缓存压缩》
文章内容：研究提出了一种在推理时通过压缩键值缓存（KV Cache）来实现超缩放的策略，显著提升了计算效率与资源利用率。
