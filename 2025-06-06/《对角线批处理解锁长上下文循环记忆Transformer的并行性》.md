模型论文地址：https://arxiv.org/abs/2506.05229

模型概述：当前标题：《Diagonal Batching Unlocks Parallelism in Recurrent Memory Transformers for Long Contexts》
中文文章标题：《对角线批处理解锁长上下文循环记忆Transformer的并行性》
文章内容：提出对角线批处理方法，通过优化内存访问模式，提升长上下文中循环记忆Transformer的并行计算效率。
